{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import (\n",
    "    text_to_wordcloud,\n",
    "    states_to_wordclouds,\n",
    "    parse_observations,\n",
    "    sample_sentence,\n",
    "    visualize_sparsities,\n",
    "    animate_emission\n",
    ")\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data_Shakespeare/shakespeare.txt')).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits, blank lines, and extra spaces. \n",
    "lines = text.splitlines()\n",
    "all_lines = []\n",
    "\n",
    "for line in lines: \n",
    "    if line.strip().isdigit() == False and len(line) > 1: \n",
    "        clean_line = line.strip().lower()\n",
    "        clean_line = re.sub('[(){}<>]', '', clean_line)\n",
    "        all_lines.append(clean_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clean poetry lines into a new file\n",
    "with open('ShakespeareLines.txt', 'w') as f:\n",
    "    for line in all_lines:\n",
    "        f.write(line)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw Shakespeare lines\n",
    "raw_text = open(os.path.join(os.getcwd(), 'ShakespeareLines.txt')).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible sequences of 40 characters\n",
    "seq_len = 40\n",
    "all_sequences = []\n",
    "raw_sequences = ''\n",
    "for i in range(seq_len, len(raw_text)):\n",
    "    seq = raw_text[i-seq_len:i+1]\n",
    "    all_sequences.append(seq)\n",
    "    raw_sequences += seq + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All unique characters\n",
    "chars = sorted(list(set(raw_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot-encoded dictionaries mapping from char to indice and from indice to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sequences with our hot-encoded dictionaries\n",
    "encoded_seq = []\n",
    "for seq in all_sequences:\n",
    "    encoded = [char_indices[char] for char in seq]\n",
    "    encoded_seq.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split encoded sequence into X and y for training the model\n",
    "encoded_seq = np.asarray(encoded_seq)\n",
    "X, y = encoded_seq[:,:-1], encoded_seq[:,-1]\n",
    "encoded_seq = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.asarray(encoded_seq)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - single layer model with 128 LSTM units and softmax output\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_len, vocab_size)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 96s - loss: 2.3375 - accuracy: 0.3267\n",
      "Epoch 2/60\n",
      " - 95s - loss: 1.9434 - accuracy: 0.4212\n",
      "Epoch 3/60\n",
      " - 95s - loss: 1.7992 - accuracy: 0.4581\n",
      "Epoch 4/60\n",
      " - 107s - loss: 1.7101 - accuracy: 0.4807\n",
      "Epoch 5/60\n",
      " - 97s - loss: 1.6444 - accuracy: 0.4970\n",
      "Epoch 6/60\n",
      " - 121s - loss: 1.5921 - accuracy: 0.5123\n",
      "Epoch 7/60\n",
      " - 111s - loss: 1.5473 - accuracy: 0.5235\n",
      "Epoch 8/60\n",
      " - 102s - loss: 1.5078 - accuracy: 0.5349\n",
      "Epoch 9/60\n",
      " - 102s - loss: 1.4736 - accuracy: 0.5439\n",
      "Epoch 10/60\n",
      " - 114s - loss: 1.4413 - accuracy: 0.5533\n",
      "Epoch 11/60\n",
      " - 118s - loss: 1.4118 - accuracy: 0.5612\n",
      "Epoch 12/60\n",
      " - 107s - loss: 1.3838 - accuracy: 0.5676\n",
      "Epoch 13/60\n",
      " - 117s - loss: 1.3567 - accuracy: 0.5755\n",
      "Epoch 14/60\n",
      " - 123s - loss: 1.3311 - accuracy: 0.5825\n",
      "Epoch 15/60\n",
      " - 127s - loss: 1.3064 - accuracy: 0.5900\n",
      "Epoch 16/60\n",
      " - 136s - loss: 1.2824 - accuracy: 0.5965\n",
      "Epoch 17/60\n",
      " - 112s - loss: 1.2595 - accuracy: 0.6042\n",
      "Epoch 18/60\n",
      " - 114s - loss: 1.2385 - accuracy: 0.6095\n",
      "Epoch 19/60\n",
      " - 114s - loss: 1.2155 - accuracy: 0.6161\n",
      "Epoch 20/60\n",
      " - 112s - loss: 1.1958 - accuracy: 0.6224\n",
      "Epoch 21/60\n",
      " - 117s - loss: 1.1762 - accuracy: 0.6287\n",
      "Epoch 22/60\n",
      " - 120s - loss: 1.1584 - accuracy: 0.6329\n",
      "Epoch 23/60\n",
      " - 106s - loss: 1.1407 - accuracy: 0.6385\n",
      "Epoch 24/60\n",
      " - 112s - loss: 1.1244 - accuracy: 0.6431\n",
      "Epoch 25/60\n",
      " - 114s - loss: 1.1087 - accuracy: 0.6492\n",
      "Epoch 26/60\n",
      " - 106s - loss: 1.0941 - accuracy: 0.6536\n",
      "Epoch 27/60\n",
      " - 101s - loss: 1.0803 - accuracy: 0.6574\n",
      "Epoch 28/60\n",
      " - 106s - loss: 1.0671 - accuracy: 0.6605\n",
      "Epoch 29/60\n",
      " - 111s - loss: 1.0558 - accuracy: 0.6647\n",
      "Epoch 30/60\n",
      " - 107s - loss: 1.0441 - accuracy: 0.6687\n",
      "Epoch 31/60\n",
      " - 105s - loss: 1.0334 - accuracy: 0.6718\n",
      "Epoch 32/60\n",
      " - 146s - loss: 1.0238 - accuracy: 0.6746\n",
      "Epoch 33/60\n",
      " - 148s - loss: 1.0134 - accuracy: 0.6763\n",
      "Epoch 34/60\n",
      " - 120s - loss: 1.0053 - accuracy: 0.6793\n",
      "Epoch 35/60\n",
      " - 111s - loss: 0.9969 - accuracy: 0.6814\n",
      "Epoch 36/60\n",
      " - 118s - loss: 0.9890 - accuracy: 0.6839\n",
      "Epoch 37/60\n",
      " - 102s - loss: 0.9830 - accuracy: 0.6847\n",
      "Epoch 38/60\n",
      " - 103s - loss: 0.9763 - accuracy: 0.6884\n",
      "Epoch 39/60\n",
      " - 113s - loss: 0.9686 - accuracy: 0.6884\n",
      "Epoch 40/60\n",
      " - 114s - loss: 0.9625 - accuracy: 0.6913\n",
      "Epoch 41/60\n",
      " - 114s - loss: 0.9584 - accuracy: 0.6932\n",
      "Epoch 42/60\n",
      " - 104s - loss: 0.9503 - accuracy: 0.6946\n",
      "Epoch 43/60\n",
      " - 108s - loss: 0.9441 - accuracy: 0.6977\n",
      "Epoch 44/60\n",
      " - 113s - loss: 0.9395 - accuracy: 0.6981\n",
      "Epoch 45/60\n",
      " - 105s - loss: 0.9372 - accuracy: 0.6977\n",
      "Epoch 46/60\n",
      " - 107s - loss: 0.9337 - accuracy: 0.6981\n",
      "Epoch 47/60\n",
      " - 117s - loss: 0.9253 - accuracy: 0.7041\n",
      "Epoch 48/60\n",
      " - 108s - loss: 0.9236 - accuracy: 0.7012\n",
      "Epoch 49/60\n",
      " - 106s - loss: 0.9191 - accuracy: 0.7041\n",
      "Epoch 50/60\n",
      " - 106s - loss: 0.9129 - accuracy: 0.7061\n",
      "Epoch 51/60\n",
      " - 99s - loss: 0.9135 - accuracy: 0.7057\n",
      "Epoch 52/60\n",
      " - 100s - loss: 0.9089 - accuracy: 0.7072\n",
      "Epoch 53/60\n",
      " - 107s - loss: 0.9006 - accuracy: 0.7105\n",
      "Epoch 54/60\n",
      " - 104s - loss: 0.8977 - accuracy: 0.7093\n",
      "Epoch 55/60\n",
      " - 104s - loss: 0.8971 - accuracy: 0.7098\n",
      "Epoch 56/60\n",
      " - 102s - loss: 0.8913 - accuracy: 0.7125\n",
      "Epoch 57/60\n",
      " - 103s - loss: 0.8930 - accuracy: 0.7104\n",
      "Epoch 58/60\n",
      " - 125s - loss: 0.8878 - accuracy: 0.7123\n",
      "Epoch 59/60\n",
      " - 107s - loss: 0.8848 - accuracy: 0.7120\n",
      "Epoch 60/60\n",
      " - 105s - loss: 0.8839 - accuracy: 0.7135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d8ebd10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X, y, epochs=60, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature - https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate poem from RNN model\n",
    "def generate_poem(seed_text, temp): \n",
    "    output = seed_text\n",
    "    lines = 1\n",
    "    for i in range(10000):\n",
    "        # Encode the current generated text\n",
    "        encoded = [char_indices[char] for char in output]\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_len, truncating='pre')\n",
    "        encoded = to_categorical(encoded, num_classes=len(char_indices))\n",
    "        # Predict next character (using temperature)\n",
    "        pred = model.predict(encoded, verbose=0)[0]\n",
    "        next_index = sample(pred, temp)\n",
    "        next_char = indices_char[next_index]\n",
    "        output += next_char\n",
    "        if next_char == '\\n': \n",
    "            lines += 1\n",
    "        if lines == 14: \n",
    "            break\n",
    "    if output[-2] in string.punctuation: \n",
    "        if output[-2] != '.' or output[-2] != '!' or output[-2] != '?': \n",
    "            return output[:-2] + '.'\n",
    "        else: \n",
    "            return output[:-1]\n",
    "\n",
    "    return output[:-1] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_1 = generate_poem(\"shall i compare thee to a summer's day?\\n\", 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "to-grable hath in that hatethrep's outblack sin elt!\n",
      "beansard which i stedpy day for wheredyy?\n",
      "thou woul yets now grows of aliquliethte.\n",
      "the ghymy', though blenss so dismine,\n",
      "him most trought me bountion forge'e foubed,\n",
      "simer to them, nim st, betreanes both's living enemen;\n",
      "gavent far thou, her coveretc uutervide\n",
      "so mar ifth from a faced do hure ther,\n",
      "sum, dost long your eye i cheple sevire cheek dit,\n",
      "and therefore shalows dwellst desseed, wilks boust kind,\n",
      "the herour in thy love no, for love impain.\n",
      "a warns a reet-bairted roched equisage;\n",
      "more ngasung tranked, upon my povaping.\n"
     ]
    }
   ],
   "source": [
    "print(poem_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_2 = generate_poem(\"shall i compare thee to a summer's day?\\n\", 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "then all my hold, len their love doth thought,\n",
      "and need berouty to time all earth.\n",
      "for then vice is do fame in all write!\n",
      "the ray by thy rich ronc, when he waknes bond,\n",
      "or he am a warthour in my mistress\n",
      "when thy heart do not precious uptainge.\n",
      "o no, that nearally dreye with the cunst\n",
      "and ear thy sweet self thy recomp love away,\n",
      "since which hair should be distind to be,\n",
      "to leaves to the romes to plece-cheat, when the receives on san,\n",
      "which i alter pentle the strangery,\n",
      "steal nothing or a cankle of the heart,\n",
      "when i have sweet heat'st this swayldy stipe.\n"
     ]
    }
   ],
   "source": [
    "print(poem_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_3 = generate_poem(\"shall i compare thee to a summer's day?\\n\", 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thou art some and more that this hell no mon.\n",
      "i know the constance of their glass and face,\n",
      "without me her be see thy sweet love have,\n",
      "when eyes it with fair after their prize\n",
      "to see the worst, whose most in one rest,\n",
      "call the fair which thou rememply it bear.\n",
      "let those that more presons the dear to be.\n",
      "for i have pleasure of thy for their prove,\n",
      "where then my love and shame all my silf,\n",
      "why whos ye worth in your sweet self to be,\n",
      "to see the form were born their state woush thy heart,\n",
      "the owe of so, mine eye is the trow of me,\n",
      "san your power to make the worst me her sight.\n"
     ]
    }
   ],
   "source": [
    "print(poem_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
